{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, dim, vocab_size):\n",
    "        super(Embeddings,self).__init__() # == super().__init__() \n",
    "        self.lut = nn.Embedding(vocab_size,dim)\n",
    "        self.d_model = dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_size = (1,6,6)\n",
    "subseq_mask = np.triu(np.ones(attention_size),k=1)\n",
    "subseq_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    attention_size = (1,size,size)\n",
    "    subseq_mask = np.triu(np.ones(attention_size),k=1)\n",
    "    return torch.from_numpy(1-subseq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 0., 0., 0., 0., 0.],\n",
       "          [1., 1., 0., 0., 0., 0.],\n",
       "          [1., 1., 1., 0., 0., 0.],\n",
       "          [1., 1., 1., 1., 0., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 1.]]], dtype=torch.float64),\n",
       " torch.Size([1, 6, 6]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trid = subsequent_mask(6)\n",
    "trid,trid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trid[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.sparse.Embedding"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(10,3)\n",
    "type(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.6451, -0.1286, -0.6585],\n",
       "        [-1.1187, -0.1429,  0.0454],\n",
       "        [ 0.7862, -0.8243, -2.0834],\n",
       "        [-0.3729,  0.5265, -0.6919],\n",
       "        [-0.1460,  0.6052, -1.1296],\n",
       "        [-0.0972,  0.1188,  0.3157],\n",
       "        [ 0.0950, -0.7536, -0.9331],\n",
       "        [-0.6951, -0.9528, -0.5592],\n",
       "        [-1.2246, -0.5633,  1.7734],\n",
       "        [ 0.7964,  1.1891,  0.5069]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.LongTensor([[1,2,3,4],[0,6,7,8]])\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = embedding(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1187, -0.1429,  0.0454],\n",
       "         [ 0.7862, -0.8243, -2.0834],\n",
       "         [-0.3729,  0.5265, -0.6919],\n",
       "         [-0.1460,  0.6052, -1.1296]],\n",
       "\n",
       "        [[ 0.6451, -0.1286, -0.6585],\n",
       "         [ 0.0950, -0.7536, -0.9331],\n",
       "         [-0.6951, -0.9528, -0.5592],\n",
       "         [-1.2246, -0.5633,  1.7734]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.LongTensor([[1,2,3,4],[0,6,7,8]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [0, 6, 7, 8]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x),type(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.LongTensor([[1,2,3,4],[0,6,7,8]]))\n",
    "d_model =512\n",
    "vocab_size = 10000\n",
    "embs = Embeddings(d_model,vocab_size= vocab_size)\n",
    "inputs_emb = embs(x)\n",
    "query = key = value = inputs_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, torch.Size([2, 4, 512]), 512)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_emb.shape[-1],inputs_emb.size(),inputs_emb.size()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask = None, dropout = None):\n",
    "    d_k = query.size()[-1]\n",
    "    scores = torch.matmul(query, key.transpose(-2,-1))/math.sqrt(d_k) #key last dim exchange with last 2nd dim\n",
    "    print(scores.size())\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==0, -1e9)\n",
    "    attn = F.softmax(scores, dim = -1)\n",
    "    print(attn.size())\n",
    "    if dropout is not None:\n",
    "        attn = dropout(attn)\n",
    "    return torch.matmul(attn, value), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = key = value = inputs_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 4])\n",
      "torch.Size([2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "values, attn = attention(query,key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def clones(module, n = 1):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(n)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head, embedding_dim, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % head == 0\n",
    "        self.d_k =   // head\n",
    "        self.head = head\n",
    "        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        # if mask\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(0) # expand the dim of mask? represent nth head in multihead\n",
    "        batch_size = query.size()[0]\n",
    "        query, key, value = [model(x).view(batch_size, -1, self.head,self.d_k).transpose(1,2) for model, x in zip(self.linears, (query, key, value))] # transpose -1 means length of sentence, finally, last two dims are length of sentence and word emb dim\n",
    "        x, self.attn = attention(query, key, value, mask = mask, dropout = self.dropout)\n",
    "        x = x.transpose(1,2).contiguous().view(batch_size, -1, self.d_k*self.head)\n",
    "        return self.linears[-1](x)            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = 8\n",
    "embedding_dim = 512\n",
    "dropout = 0.2\n",
    "#input\n",
    "query = key = value = inputs_emb\n",
    "mask = Variable(torch.zeros(8,4,4)) # number of head, matrix dim\n",
    "mask1 = mask.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)\n",
    "batch_size = query.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([2, 4, 512]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = linears[0](query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = aa.view(batch_size, -1, head,embedding_dim//head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8, 64])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1, key1, value1 = [model(x).view(batch_size, -1, head,embedding_dim//head).transpose(1,2) for model, x in zip(linears, (query, key, value))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 4, 64])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 512]), torch.Size([2, 8, 4, 64]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape,query1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask = None, dropout = None):\n",
    "    d_k = query.size()[-1]\n",
    "    #print(query.size())\n",
    "    scores = torch.matmul(query, key.transpose(-2,-1))/math.sqrt(d_k) #key last dim exchange with last 2nd dim\n",
    "    #print(scores.size())\n",
    "    if mask is not None:\n",
    "        #print(mask.size())\n",
    "        scores = scores.masked_fill(mask==0, -1e9)\n",
    "        #print(scores)\n",
    "    attn = F.softmax(scores, dim = -1)\n",
    "    #print(attn.size())\n",
    "    if dropout is not None:\n",
    "        attn = dropout(attn)\n",
    "    return torch.matmul(attn, value), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1.shape == key1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4, 64])\n",
      "torch.Size([2, 8, 4, 4])\n",
      "torch.Size([1, 8, 4, 4])\n",
      "tensor([[[[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "torch.Size([2, 8, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "x, attn = attention(query1, key1, value1, mask = mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(linears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(head, embedding_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4, 64])\n",
      "torch.Size([2, 8, 4, 4])\n",
      "torch.Size([1, 8, 4, 4])\n",
      "tensor([[[[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "torch.Size([2, 8, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "mha_result = mha(query,key,value,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.6542,  5.3653,  3.0327,  ...,  1.3045,  6.9633, -1.8286],\n",
       "         [ 3.8160, -0.9026,  7.7298,  ...,  0.1826,  4.8034, -0.0115],\n",
       "         [ 3.6228,  4.8580,  5.5297,  ...,  4.4123,  4.6706, -0.6531],\n",
       "         [ 1.3359,  4.3676,  4.0880,  ...,  1.5854,  3.6474, -1.6719]],\n",
       "\n",
       "        [[ 1.0590, -4.1020, -0.7624,  ..., -8.0675,  1.7987,  4.2940],\n",
       "         [-3.5120, -6.5496,  0.5052,  ..., -7.4359, -0.8139,  3.2296],\n",
       "         [-3.1542, -5.1074,  2.7063,  ..., -8.7210,  0.7330,  3.4519],\n",
       "         [-0.1865,  0.7082,  0.8235,  ..., -7.9318, -1.3038,  0.4232]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, attn_output):\n",
    "        return self.w2(self.dropout(F.relu(self.w1(attn_output))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "d_ff = 128\n",
    "x = mha_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = PositionWiseFeedForward(d_model,d_ff)\n",
    "ffn_result = ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = nn.Parameter(torch.ones(1))\n",
    "ones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_k, esp = 1e-6):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "        self.esp = esp\n",
    "        self.a1 = nn.Parameter(torch.ones(d_k))\n",
    "        self.b1 = nn.Parameter(torch.zeros(d_k))\n",
    "    def forward(self, x):\n",
    "        x_mean = x.mean(-1,keepdim=True) # word embedding mean\n",
    "        x_std = x.std(-1,keepdim=True)\n",
    "        return self.a1*(x-x_mean)/(x_std+self.esp) +self.b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = d_model = 512\n",
    "eps = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(d_model,eps)\n",
    "normalized_r = ln(ffn_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'leet', 'code']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"/leet/code\".split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubLayerConnection(nn.Module):\n",
    "    def __init__(self, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.d_model = d_model\n",
    "        self.norm = LayerNorm(d_model)\n",
    "    def forward(self, sublayer, x):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.LongTensor([[1,2,3,4],[0,6,7,8]]))\n",
    "d_model =512\n",
    "vocab_size = 10000\n",
    "embs = Embeddings(d_model,vocab_size= vocab_size)\n",
    "inputs_emb = embs(x)\n",
    "query = key = value = inputs_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = d_model = 512\n",
    "head = 8\n",
    "dropout = 0.2\n",
    "mask = Variable(torch.zeros(8,4,4))\n",
    "self_attn = MultiHeadAttention(head, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4, 64])\n",
      "torch.Size([2, 8, 4, 4])\n",
      "torch.Size([1, 8, 4, 4])\n",
      "tensor([[[[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "torch.Size([2, 8, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sublayer = lambda x:self_attn(x,x,x, mask)\n",
    "sc = SubLayerConnection(d_model, dropout)\n",
    "sc_result = sc(sublayer, inputs_emb)\n",
    "sc_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
